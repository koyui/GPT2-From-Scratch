DATA:
  # General settings for the dataset
  data_root: ./deps/reviews.csv
  val_ratio: 0.1
  batch_size: 32
  max_tokens: 256

  # Settings for the tokenizer
  ################################################## BBPE
  # tokenizer: bbpe
  # save_merge:
  # # save_merge: ./deps
  # # from_file:
  # from_file: ./deps/merge_rule_1024.json
  ################################################## BERT
  tokenizer: bert
  save_merge:
  # save_merge: ./deps
  # from_file:
  from_file: ./deps/bert_map_2550.json

MODEL:
  # phase: train
  phase: test
  # phase: evaluation

  # vocab_size: 1024
  vocab_size: 2550 # for bert

  n_emb: 96
  n_head: 3
  n_layer: 6
  max_tokens: 256
  topk: 3

TRAIN:
  lr: 1e-4
  weight_decay: 1e-4
  max_training_epochs: 150
  save_per: 6
  model_save: ./ckpts/gpt2
  from_pretrained: 
  with_monitor: True

TEST:
  # from_pretrained: ./ckpts/gpt2_20250329_205802/36.pth
  from_pretrained: ./ckpts/gpt2_20250329_143840/12.pth
  from_file: ./test/input.txt
  result_save: ./test/result.txt
  vis: './tmp/vis'

EVALUATE:
  # evaluate_folder:
  evaluate_folder: ./ckpts/gpt2_20250402_022928
  from_pretrained: ./ckpts/gpt2_20250329_172710/66.pth
  eval_length: 20