import os
import time
import wandb
import torch
import torch.nn as nn
import torch.nn.functional as F

import os.path as osp

from torch.utils.data.dataloader import DataLoader
from tqdm import trange, tqdm
from omegaconf import OmegaConf

from models.model import GPT2
from data.utils import collate_fn
from data.dataset import GPT2Dataset
from utils.bbpe import BBPE

device = "cpu"
if torch.cuda.is_available():
    device = "cuda"

class Trainer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.bbpe = BBPE(config.MODEL.vocab_size)
        # bbpe load or build in the dataset initialization
        self.model = GPT2(config.MODEL, self.bbpe).to(device)
        if self.config.MODEL.phase == 'train':
            self.dataset = GPT2Dataset(config.DATA, self.bbpe, device)
            self.dataloader = DataLoader(
                self.dataset,
                config.DATA.batch_size,
                shuffle=False, 
                drop_last=False, 
                collate_fn=collate_fn
            )
            if self.config.TRAIN.from_pretrained:
                self.load_model(self.config.TRAIN.from_pretrained)
                
            self.time_stamp = time.strftime("%Y%m%d_%H%M%S")
            if self.config.TRAIN.with_monitor:
                wandb.init(project="koyui_GPT2", name="train_" + self.time_stamp, config=dict(self.config))
                
        elif self.config.MODEL.phase == 'test':
            self.model.eval()
             
    def load_model(self, model_path):
        milestone = torch.load(model_path, map_location=device)
        self.model.load_state_dict(milestone)

    def save_model(self, epoch):
        save_dir = self.config.TRAIN.model_save + '_' + self.time_stamp
        os.makedirs(save_dir, exist_ok=True)
        
        save_path = osp.join(save_dir, f'{epoch}.pth')
        torch.save(self.model.state_dict(), save_path)
        
    def generate(self, x):
        # TODO: Real generate with batch, now batch generate assumes all sentences in the same size without padding, 
        while x.shape[1] < self.config.DATA.max_tokens:
            with torch.no_grad():
                logits = self.model(x)  # (B, T, vocab_size)
                logits = logits[:, -1, :]   # (B, vocab_size)
                probs = F.softmax(logits, dim=-1)
                topk_probs, topk_indices = torch.topk(probs, self.config.MODEL.topk, dim=-1)  # (B, topk), (B, topk)
                idxs = torch.multinomial(topk_probs, 1)    # (B, 1)
                token_id = torch.gather(topk_indices, -1, idxs)
                x = torch.cat([x, token_id], dim=-1)
        return x
    
    def train(self):
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.config.TRAIN.lr)
        training_loop = trange(self.config.TRAIN.max_training_epochs)
        for epoch in training_loop:
            loss_list = []
            for batch in tqdm(self.dataloader):
                optimizer.zero_grad()
                _, loss = self.model(batch["input"], batch["target"], batch["mask"])
                loss_list.append(loss.item())
                loss.backward()
                optimizer.step()
                
            loss_per_element = sum(loss_list)/len(loss_list)
            wandb.log({"loss_per_element": loss_per_element, "epoch": epoch})
            training_loop.set_description(f'Loss per element: {loss_per_element}')
            if epoch % self.config.TRAIN.save_per == 0:
                self.save_model(epoch)
    
    def batch_decode(self, batch):
        """
            batch: A batch of tokens generated by GPT2 model.
            Return a list of text decoded by our bbpe tokenizer.
        """
        
        if len(batch.shape) == 1:   # (T, )
            batch = batch.unsqueeze(0)  # (B, T)
        batch = batch.to_list()
        text_list = []
        for tokens in batch:
            eos_pos = len(tokens)
            sep_pos = None
            for pos, token in enumerate(tokens):
                if token == self.bbpe.sep_token:
                    sep_pos = pos + 1
                elif token == self.bbpe.eos_token:
                    eos_pos = pos
                    break
            text_list.append(self.bbpe.decode(tokens[sep_pos:eos_pos]))
            
        return text_list
    
    def test(self):
        with open(config.TEST.from_file, 'r') as f:
            text_list = f.readlines()
        results = []
        for text in text_list:
            tokens = self.bbpe.encode(text)
            tokens.append(self.bbpe.sep_token)
            x = torch.as_tensor(tokens).unsqueeze(0).long().to(device)
            text = self.generate(x)
            decoded_text = self.batch_decode(decoded_text)
            results += decoded_text
        with open(self.config.TEST.result_save, 'w') as f:
            f.writelines(results)
        print(f"Test done, save to {self.config.TEST.result_save}")
        
                
if __name__ == "__main__":
    config = OmegaConf.load('./configs/config.yaml')
    trainer = Trainer(config)
    if config.MODEL.phase == 'train':
        trainer.train()
    elif config.MODEL.phase == 'test':
        trainer.test()
    